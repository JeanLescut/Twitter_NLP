{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jlescutmuller/.local/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tweepy\n",
    "# tweepy.debug(False)\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweepy Documentation: https://docs.tweepy.org/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consumer Keys\n",
    "consumer_key = '' # Also called \"API Key\" under \"Consumer Keys\"\n",
    "consumer_secret = '' # Also called \"Secret Key\" under \"Consumer Keys\"\n",
    "\n",
    "# Authentification token :\n",
    "bearer_token = ''\n",
    "access_token = ''\n",
    "access_token_secret = 'Q'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweeter API v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token=bearer_token,\n",
    "                       consumer_key=consumer_key,\n",
    "                       consumer_secret=consumer_secret,\n",
    "                       access_token=access_token,\n",
    "                       access_token_secret=access_token_secret) # maybe not all of them are necessary...\n",
    "client.search_recent_tweets(query='toto OR titi') # support operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = tweepy.Client(bearer_token=bearer_token,\n",
    "                       consumer_key=consumer_key,\n",
    "                       consumer_secret=consumer_secret,\n",
    "                       access_token=access_token,\n",
    "                       access_token_secret=access_token_secret) # maybe not all of them are necessary...\n",
    "l = client.search_recent_tweets(query='toto OR titi', place_fields=['geo']) # support operators\n",
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweeter API v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters, digits\n",
    "import numpy as np\n",
    "import os\n",
    "def randID(size = 4) :\n",
    "    return ''.join(np.random.choice(list(ascii_letters + digits), size=size, replace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(mode) :\n",
    "    version = 'v6'\n",
    "    global t\n",
    "    while True :\n",
    "        if mode == 'initialization' :\n",
    "            since_id = None\n",
    "            max_id = None\n",
    "        else :\n",
    "            print('Reading df from disk...')\n",
    "            df = pd.read_pickle(f'./data/tweets_raw/{version}/df.pkl')\n",
    "            if mode == 'backward_fill' :\n",
    "                since_id = df['id'].min()-int(1e15)\n",
    "                max_id = df['id'].min()-1\n",
    "            elif mode == 'forward_fill' :\n",
    "                since_id = df['id'].max()+1\n",
    "                max_id   = df['id'].max()+int(1e15)\n",
    "            print(f'since_id = {since_id:,d}')\n",
    "#             print(f'since_id (last 16 digits) = {int(since_id%1e16)}')\n",
    "            print(f'  max_id = {max_id:,d}')\n",
    "#             print(f'max_id (last 16 digits) = {int(max_id%1e16)}')\n",
    "            \n",
    "        curs = tweepy.Cursor(api.search_tweets,\n",
    "                             q='\"data science\"',\n",
    "                             since_id = since_id,\n",
    "                             max_id = max_id,\n",
    "                             count = 100,\n",
    "        #                      geocode=\"39.099724,-94.578331,11000km\",  # LaLo of Kansas City (~center of USA)\n",
    "                             lang=\"en\").items(17000)\n",
    "\n",
    "        tweet_data = []\n",
    "#         place_data = []\n",
    "        i = 0\n",
    "        while True :\n",
    "            try : \n",
    "                t = curs.next()\n",
    "                if i%1000 == 0 :\n",
    "                    print(f'i={i}...')\n",
    "            except StopIteration as e :\n",
    "                print(f'Done ! (i={i})')\n",
    "                break\n",
    "            except tweepy.errors.TooManyRequests as e :\n",
    "                # Rate limit is 300 requests every 15 min : https://developer.twitter.com/en/docs/twitter-api/rate-limits\n",
    "                # Because it's 100 tweets per request --> 30,000 tweets every 15min.\n",
    "                print(f'TooManyRequest Error (i={i}). Sleeping 2min and retrying...')\n",
    "                time.sleep(120)\n",
    "                continue\n",
    "            i += 1\n",
    "            \n",
    "#             geo_location_info = None\n",
    "#             place_to_add = None\n",
    "#             if t.geo is not None and type(t.geo) is dict and t.geo['type']=='Point' :\n",
    "#                 geo_location_info = {'lola': t.geo['coordinates']}\n",
    "#             elif type(t.place) is dict :\n",
    "#                 geo_location_info = {'place_id': t.place.id}\n",
    "#                 place_to_add = t.place\n",
    "#             elif (t.user.location is not None and len(t.user.location) > 0) :\n",
    "#                 geo_location_info = {'user_location': t.user.location}\n",
    "#             some_location_data = (\n",
    "#                 (t.geo is not None) \n",
    "#                 or (t.coordinates is not None) \n",
    "#                 or (t.place is not None) \n",
    "#                 or (t.user.location is not None and len(t.user.location) > 0)\n",
    "#             )\n",
    "#             if geo_location_info is None and some_location_data :\n",
    "#                 print(t._json)\n",
    "#                 raise Exception('some geolocation data !!')\n",
    "                \n",
    "            tweet_data.append((\n",
    "                t.id,\n",
    "                t.created_at,\n",
    "                t.text,\n",
    "                t.truncated,\n",
    "                [h['text'] for h in t.entities['hashtags']],\n",
    "                t.geo,\n",
    "                t.place,\n",
    "                t.coordinates,\n",
    "                t.user.location\n",
    "            ))\n",
    "\n",
    "            \n",
    "        print(f'We evaluated {i} tweets in total')\n",
    "        print(f'We selected {len(tweet_data)} tweets')\n",
    "        if i <= 1000 :\n",
    "            print(f'Not enough. Waiting 30 min before restarting the loop...')\n",
    "            time.sleep(60*30) # wait 30 min\n",
    "            continue\n",
    "\n",
    "        df_extra = pd.DataFrame(tweet_data,\n",
    "                                columns=['id', 'created_at', 'text', 'truncated', 'hashtags', \n",
    "                                         'geo', 'place', 'coordinates', 'user_location'])\n",
    "        print(f'df_extra[\"id\"].min() = {df_extra[\"id\"].min():,d}')\n",
    "        print(f'df_extra[\"id\"].max() = {df_extra[\"id\"].max():,d}')\n",
    "\n",
    "        if mode == 'initialization' :\n",
    "            df = df_extra\n",
    "        else :\n",
    "            df = pd.concat([df, df_extra])\n",
    "        print(f'df[\"id\"].min() = {df[\"id\"].min():,d}')\n",
    "        print(f'df[\"id\"].max() = {df[\"id\"].max():,d}')\n",
    "        \n",
    "        df = df.drop_duplicates('text') # Remove duplicates...\n",
    "        print(df.shape)\n",
    "\n",
    "        os.makedirs(f'./data/tweets_raw/{version}', exist_ok=True) # Just in case we are initializing\n",
    "        df.to_pickle(f'./data/tweets_raw/{version}/df.pkl')\n",
    "        if mode == 'initialization' :\n",
    "            return 'Initialization done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading df from disk...\n",
      "since_id = 1,419,398,875,077,652,487\n",
      "  max_id = 1,420,398,875,077,652,486\n",
      "i=0...\n",
      "i=1000...\n",
      "Done ! (i=1363)\n",
      "We evaluated 1363 tweets in total\n",
      "We selected 1363 tweets\n",
      "df_extra[\"id\"].min() = 1,419,398,965,880,033,281\n",
      "df_extra[\"id\"].max() = 1,419,598,392,020,271,105\n",
      "df[\"id\"].min() = 1,416,416,532,595,412,993\n",
      "df[\"id\"].max() = 1,419,598,392,020,271,105\n",
      "(11883, 9)\n",
      "Reading df from disk...\n",
      "since_id = 1,419,598,392,020,271,106\n",
      "  max_id = 1,420,598,392,020,271,105\n",
      "Done ! (i=0)\n",
      "We evaluated 0 tweets in total\n",
      "We selected 0 tweets\n",
      "Not enough. Waiting 30 min before restarting the loop...\n",
      "Reading df from disk...\n",
      "since_id = 1,419,598,392,020,271,106\n",
      "  max_id = 1,420,598,392,020,271,105\n",
      "i=0...\n",
      "Done ! (i=69)\n",
      "We evaluated 69 tweets in total\n",
      "We selected 69 tweets\n",
      "Not enough. Waiting 30 min before restarting the loop...\n",
      "Reading df from disk...\n",
      "since_id = 1,419,598,392,020,271,106\n",
      "  max_id = 1,420,598,392,020,271,105\n",
      "i=0...\n",
      "Done ! (i=156)\n",
      "We evaluated 156 tweets in total\n",
      "We selected 156 tweets\n",
      "Not enough. Waiting 30 min before restarting the loop...\n",
      "Reading df from disk...\n",
      "since_id = 1,419,598,392,020,271,106\n",
      "  max_id = 1,420,598,392,020,271,105\n",
      "i=0...\n",
      "Done ! (i=264)\n",
      "We evaluated 264 tweets in total\n",
      "We selected 264 tweets\n",
      "Not enough. Waiting 30 min before restarting the loop...\n",
      "Reading df from disk...\n",
      "since_id = 1,419,598,392,020,271,106\n",
      "  max_id = 1,420,598,392,020,271,105\n",
      "i=0...\n",
      "Done ! (i=345)\n",
      "We evaluated 345 tweets in total\n",
      "We selected 345 tweets\n",
      "Not enough. Waiting 30 min before restarting the loop...\n"
     ]
    }
   ],
   "source": [
    "run(mode='forward_fill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m81"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
